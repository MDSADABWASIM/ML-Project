{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitbaseconda0633cb87348e4e1f83d9c32f4c55bf11",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renewable Energy requirements prediction project\n",
    "\n",
    "### Context:\n",
    "    This will act as the base data for the investigation into the possible solutions for the UK energy requirements\n",
    "\n",
    "### Content:\n",
    "     Data from the UK statistics on renewable energy generation.\n",
    "\n",
    "Acknowledgements\n",
    "https://www.gov.uk/government/statistics/regional-renewable-statistics7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('energy').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('../datasets/renewable_energy.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+---+----+--------------------+-----+--------------+--------+-------+------------+---------------------------------+-------+\n|_c0|Year|              Region|Wind2|Wave and tidal|Solar PV|  Hydro|Landfill gas|Other bioenergy (incl Sewage gas3|  Total|\n+---+----+--------------------+-----+--------------+--------+-------+------------+---------------------------------+-------+\n|  1|2003|             England|349.2|           0.0|     0.0|   25.3|     2,899.0|                          2,716.9|5,990.4|\n|  2|2003|       East Midlands|  1.3|           0.0|     0.0|    5.5|       223.1|                            202.7|  432.6|\n|  3|2003|     East of England| 19.8|           0.0|     0.0|      -|       756.4|                            748.8|1,525.0|\n|  4|2003|          North East| 28.6|           0.0|     0.0|      …|        89.9|                            139.0|  257.5|\n|  5|2003|          North West|120.8|           0.0|     0.0|    2.0|       519.4|                            216.3|  858.5|\n|  6|2003|              London|    -|           0.0|     0.0|      -|           -|                            438.8|  438.8|\n|  7|2003|          South East|  3.5|           0.0|     0.0|      -|       602.3|                            187.0|  792.8|\n|  8|2003|          South West| 95.8|           0.0|     0.0|   16.4|       249.7|                             90.7|  452.6|\n|  9|2003|       West Midlands|    -|           0.0|     0.0|    1.4|       224.6|                            355.3|  581.3|\n| 10|2003|Yorkshire and the...| 79.4|           0.0|     0.0|      …|       233.6|                            338.3|  651.3|\n| 11|2003|    Northern Ireland| 96.3|           0.0|     0.0|    6.7|           -|                              1.3|  104.3|\n| 12|2003|            Scotland|448.9|           0.0|     0.0|2,902.0|       228.0|                            145.5|3,724.4|\n| 13|2003|               Wales|391.0|           0.0|     0.0|  194.7|       149.3|                             33.9|  768.9|\n| 14|2003|        Other Sites4|    -|           0.0|     2.9|    8.8|           -|                                -|   11.7|\n| 15|2004|             England|387.8|           0.0|     0.0|   70.2|     3,501.1|                          3,160.5|7,119.6|\n| 16|2004|       East Midlands|    …|           0.0|     0.0|   11.1|       287.2|                             79.0|  377.3|\n| 17|2004|     East of England| 51.0|           0.0|     0.0|      -|       835.9|                            687.1|1,574.0|\n| 18|2004|          North East| 43.7|           0.0|     0.0|   16.9|       132.3|                            212.6|  405.5|\n| 19|2004|          North West|117.7|           0.0|     0.0|    8.2|       687.9|                            254.2|1,068.0|\n| 20|2004|              London|    -|           0.0|     0.0|      -|           -|                            434.8|  434.8|\n+---+----+--------------------+-----+--------------+--------+-------+------------+---------------------------------+-------+\nonly showing top 20 rows\n\n"
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "root\n |-- _c0: integer (nullable = true)\n |-- Year: integer (nullable = true)\n |-- Region: string (nullable = true)\n |-- Wind2: string (nullable = true)\n |-- Wave and tidal: double (nullable = true)\n |-- Solar PV: string (nullable = true)\n |-- Hydro: string (nullable = true)\n |-- Landfill gas: string (nullable = true)\n |-- Other bioenergy (incl Sewage gas3: string (nullable = true)\n |-- Total: string (nullable = true)\n\n"
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for NAN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function use to print feature with null values and null count \n",
    "def null_value_count(df):\n",
    "  null_columns_counts = []\n",
    "  numRows = df.count()\n",
    "  for k in df.columns:\n",
    "    nullRows = df.where(df[k].isNull()).count()\n",
    "    if(nullRows > 0):\n",
    "      temp = k,nullRows\n",
    "      null_columns_counts.append(temp)\n",
    "  return(null_columns_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "There are 0 null data points in dataset\n"
    }
   ],
   "source": [
    "null_values = null_value_count(df)\n",
    "print(\"There are %s null data points in dataset\"%len(null_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing and cleaning:\n",
    "    replace '…' and '-' to 0\n",
    "    and removing ',' from numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+---+----+--------------------+-----+--------------+--------+-------+------------+---------------------------------+-------+\n|_c0|Year|              Region|Wind2|Wave and tidal|Solar PV|  Hydro|Landfill gas|Other bioenergy (incl Sewage gas3|  Total|\n+---+----+--------------------+-----+--------------+--------+-------+------------+---------------------------------+-------+\n|  1|2003|             England|349.2|           0.0|     0.0|   25.3|     2,899.0|                          2,716.9|5,990.4|\n|  2|2003|       East Midlands|  1.3|           0.0|     0.0|    5.5|       223.1|                            202.7|  432.6|\n|  3|2003|     East of England| 19.8|           0.0|     0.0|      0|       756.4|                            748.8|1,525.0|\n|  4|2003|          North East| 28.6|           0.0|     0.0|      0|        89.9|                            139.0|  257.5|\n|  5|2003|          North West|120.8|           0.0|     0.0|    2.0|       519.4|                            216.3|  858.5|\n|  6|2003|              London|    0|           0.0|     0.0|      0|           0|                            438.8|  438.8|\n|  7|2003|          South East|  3.5|           0.0|     0.0|      0|       602.3|                            187.0|  792.8|\n|  8|2003|          South West| 95.8|           0.0|     0.0|   16.4|       249.7|                             90.7|  452.6|\n|  9|2003|       West Midlands|    0|           0.0|     0.0|    1.4|       224.6|                            355.3|  581.3|\n| 10|2003|Yorkshire and the...| 79.4|           0.0|     0.0|      0|       233.6|                            338.3|  651.3|\n| 11|2003|    Northern Ireland| 96.3|           0.0|     0.0|    6.7|           0|                              1.3|  104.3|\n| 12|2003|            Scotland|448.9|           0.0|     0.0|2,902.0|       228.0|                            145.5|3,724.4|\n| 13|2003|               Wales|391.0|           0.0|     0.0|  194.7|       149.3|                             33.9|  768.9|\n| 14|2003|        Other Sites4|    0|           0.0|     2.9|    8.8|           0|                                0|   11.7|\n| 15|2004|             England|387.8|           0.0|     0.0|   70.2|     3,501.1|                          3,160.5|7,119.6|\n| 16|2004|       East Midlands|    0|           0.0|     0.0|   11.1|       287.2|                             79.0|  377.3|\n| 17|2004|     East of England| 51.0|           0.0|     0.0|      0|       835.9|                            687.1|1,574.0|\n| 18|2004|          North East| 43.7|           0.0|     0.0|   16.9|       132.3|                            212.6|  405.5|\n| 19|2004|          North West|117.7|           0.0|     0.0|    8.2|       687.9|                            254.2|1,068.0|\n| 20|2004|              London|    0|           0.0|     0.0|      0|           0|                            434.8|  434.8|\n+---+----+--------------------+-----+--------------+--------+-------+------------+---------------------------------+-------+\nonly showing top 20 rows\n\n"
    }
   ],
   "source": [
    "df = df.replace('…','0')\n",
    "df = df.replace('-','0')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['_c0',\n 'Year',\n 'Region',\n 'Wind2',\n 'Wave and tidal',\n 'Solar PV',\n 'Hydro',\n 'Landfill gas',\n 'Other bioenergy (incl Sewage gas3',\n 'Total']"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Replacing ',' to '' from all numerical columns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace,col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.withColumn('Total_',regexp_replace(col('Total'),',','')).withColumn('Landfill_gas',regexp_replace(col('Landfill gas'),',','')).withColumn('Other',regexp_replace(col('Other bioenergy (incl Sewage gas3'),',','')).withColumn('Solar_PV',regexp_replace(col('Solar PV'),',','')).withColumn('Wind_2',regexp_replace(col('Wind2'),',','')).withColumn('Hydro_',regexp_replace(col('Hydro'),',',''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+-------+------+\n|  Hydro|Hydro_|\n+-------+------+\n|   25.3|  25.3|\n|    5.5|   5.5|\n|      0|     0|\n|      0|     0|\n|    2.0|   2.0|\n|      0|     0|\n|      0|     0|\n|   16.4|  16.4|\n|    1.4|   1.4|\n|      0|     0|\n|    6.7|   6.7|\n|2,902.0|2902.0|\n|  194.7| 194.7|\n|    8.8|   8.8|\n|   70.2|  70.2|\n|   11.1|  11.1|\n|      0|     0|\n|   16.9|  16.9|\n|    8.2|   8.2|\n|      0|     0|\n+-------+------+\nonly showing top 20 rows\n\n"
    }
   ],
   "source": [
    "data.select('Hydro','Hydro_').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+----+--------------------+--------------+------+------------+------+--------+------+------+\n|Year|              Region|Wave and tidal|Total_|Landfill_gas| Other|Solar_PV|Wind_2|Hydro_|\n+----+--------------------+--------------+------+------------+------+--------+------+------+\n|2003|             England|           0.0|5990.4|      2899.0|2716.9|     0.0| 349.2|  25.3|\n|2003|       East Midlands|           0.0| 432.6|       223.1| 202.7|     0.0|   1.3|   5.5|\n|2003|     East of England|           0.0|1525.0|       756.4| 748.8|     0.0|  19.8|     0|\n|2003|          North East|           0.0| 257.5|        89.9| 139.0|     0.0|  28.6|     0|\n|2003|          North West|           0.0| 858.5|       519.4| 216.3|     0.0| 120.8|   2.0|\n|2003|              London|           0.0| 438.8|           0| 438.8|     0.0|     0|     0|\n|2003|          South East|           0.0| 792.8|       602.3| 187.0|     0.0|   3.5|     0|\n|2003|          South West|           0.0| 452.6|       249.7|  90.7|     0.0|  95.8|  16.4|\n|2003|       West Midlands|           0.0| 581.3|       224.6| 355.3|     0.0|     0|   1.4|\n|2003|Yorkshire and the...|           0.0| 651.3|       233.6| 338.3|     0.0|  79.4|     0|\n|2003|    Northern Ireland|           0.0| 104.3|           0|   1.3|     0.0|  96.3|   6.7|\n|2003|            Scotland|           0.0|3724.4|       228.0| 145.5|     0.0| 448.9|2902.0|\n|2003|               Wales|           0.0| 768.9|       149.3|  33.9|     0.0| 391.0| 194.7|\n|2003|        Other Sites4|           0.0|  11.7|           0|     0|     2.9|     0|   8.8|\n|2004|             England|           0.0|7119.6|      3501.1|3160.5|     0.0| 387.8|  70.2|\n|2004|       East Midlands|           0.0| 377.3|       287.2|  79.0|     0.0|     0|  11.1|\n|2004|     East of England|           0.0|1574.0|       835.9| 687.1|     0.0|  51.0|     0|\n|2004|          North East|           0.0| 405.5|       132.3| 212.6|     0.0|  43.7|  16.9|\n|2004|          North West|           0.0|1068.0|       687.9| 254.2|     0.0| 117.7|   8.2|\n|2004|              London|           0.0| 434.8|           0| 434.8|     0.0|     0|     0|\n+----+--------------------+--------------+------+------------+------+--------+------+------+\nonly showing top 20 rows\n\n"
    }
   ],
   "source": [
    "columns_to_drop = ['Total','Landfill gas','Other bioenergy (incl Sewage gas3','Solar PV','Wind2','Hydro','_c0']\n",
    "data = data.drop(*columns_to_drop)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "root\n |-- Year: integer (nullable = true)\n |-- Region: string (nullable = true)\n |-- Wave and tidal: double (nullable = true)\n |-- Total_: string (nullable = true)\n |-- Landfill_gas: string (nullable = true)\n |-- Other: string (nullable = true)\n |-- Solar_PV: string (nullable = true)\n |-- Wind_2: string (nullable = true)\n |-- Hydro_: string (nullable = true)\n\n"
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting all Numerical columns to double type (it's String now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "root\n |-- Year: integer (nullable = true)\n |-- Region: string (nullable = true)\n |-- Wave and tidal: double (nullable = true)\n |-- Total_: double (nullable = true)\n |-- Landfill_gas: double (nullable = true)\n |-- Other: double (nullable = true)\n |-- Solar_PV: double (nullable = true)\n |-- Wind_2: double (nullable = true)\n |-- Hydro_: double (nullable = true)\n\n"
    }
   ],
   "source": [
    "final_data = data.withColumn('Total_',data['Total_'].cast(DoubleType())).withColumn('Landfill_gas',data['Landfill_gas'].cast(DoubleType())).withColumn('Other',data['Other'].cast(DoubleType())).withColumn('Solar_PV',data['Solar_PV'].cast(DoubleType())).withColumn('Wind_2',data['Wind_2'].cast(DoubleType())).withColumn('Hydro_',data['Hydro_'].cast(DoubleType()))\n",
    "\n",
    "final_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing my Region column to numerical column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer,VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol='Region',outputCol='Region_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = indexer.fit(final_data).transform(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+--------------------+------------+\n|              Region|Region_index|\n+--------------------+------------+\n|             England|        12.0|\n|       East Midlands|        13.0|\n|     East of England|         1.0|\n|          North East|         9.0|\n|          North West|         7.0|\n|              London|         0.0|\n|          South East|         8.0|\n|          South West|         2.0|\n|       West Midlands|         3.0|\n|Yorkshire and the...|         5.0|\n|    Northern Ireland|        10.0|\n|            Scotland|         6.0|\n|               Wales|         4.0|\n|        Other Sites4|        11.0|\n|             England|        12.0|\n|       East Midlands|        13.0|\n|     East of England|         1.0|\n|          North East|         9.0|\n|          North West|         7.0|\n|              London|         0.0|\n+--------------------+------------+\nonly showing top 20 rows\n\n"
    }
   ],
   "source": [
    "final_data.select('Region','Region_index').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['Year',\n 'Wave and tidal',\n 'Total_',\n 'Landfill_gas',\n 'Other',\n 'Solar_PV',\n 'Wind_2',\n 'Hydro_',\n 'Region_index']"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data = final_data.drop('Region')\n",
    "final_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assemble the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['Year','Wave and tidal','Landfill_gas',\n",
    " 'Other',\n",
    " 'Solar_PV',\n",
    " 'Wind_2',\n",
    " 'Hydro_',\n",
    " 'Region_index'],outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = assembler.transform(final_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting only features and label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+------+--------------------+\n|Total_|            features|\n+------+--------------------+\n|5990.4|[2003.0,0.0,2899....|\n| 432.6|[2003.0,0.0,223.1...|\n|1525.0|[2003.0,0.0,756.4...|\n| 257.5|[2003.0,0.0,89.9,...|\n| 858.5|[2003.0,0.0,519.4...|\n| 438.8|(8,[0,3],[2003.0,...|\n| 792.8|[2003.0,0.0,602.3...|\n| 452.6|[2003.0,0.0,249.7...|\n| 581.3|[2003.0,0.0,224.6...|\n| 651.3|[2003.0,0.0,233.6...|\n| 104.3|[2003.0,0.0,0.0,1...|\n|3724.4|[2003.0,0.0,228.0...|\n| 768.9|[2003.0,0.0,149.3...|\n|  11.7|(8,[0,4,6,7],[200...|\n|7119.6|[2004.0,0.0,3501....|\n| 377.3|[2004.0,0.0,287.2...|\n|1574.0|[2004.0,0.0,835.9...|\n| 405.5|[2004.0,0.0,132.3...|\n|1068.0|[2004.0,0.0,687.9...|\n| 434.8|(8,[0,3],[2004.0,...|\n+------+--------------------+\nonly showing top 20 rows\n\n"
    }
   ],
   "source": [
    "final_data = final_data.select('Total_','features')\n",
    "final_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol='features',outputCol='scaled_features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Total column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean,stddev\n",
    "\n",
    "mean_total, sttdev_total = final_data.select(mean(\"Total_\"), stddev(\"Total_\")).first()\n",
    "final_data=final_data.withColumn(\"scaled_total\", (col(\"Total_\") - mean_total) / sttdev_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+------+--------------------+--------------------+\n|Total_|            features|        scaled_total|\n+------+--------------------+--------------------+\n|5990.4|[2003.0,0.0,2899....|  1.5486085855875653|\n| 432.6|[2003.0,0.0,223.1...| -0.3823508034399062|\n|1525.0|[2003.0,0.0,756.4...|-0.00281572902671...|\n| 257.5|[2003.0,0.0,89.9,...|-0.44318620396146335|\n| 858.5|[2003.0,0.0,519.4...|-0.23437937521528493|\n| 438.8|(8,[0,3],[2003.0,...|-0.38019672301024515|\n| 792.8|[2003.0,0.0,602.3...|-0.25720567912314507|\n| 452.6|[2003.0,0.0,249.7...|-0.37540215689261236|\n| 581.3|[2003.0,0.0,224.6...| -0.3306876163607769|\n| 651.3|[2003.0,0.0,233.6...| -0.3063673534452486|\n| 104.3|[2003.0,0.0,0.0,1...| -0.4964128365137338|\n|3724.4|[2003.0,0.0,228.0...|   0.761326931779179|\n| 768.9|[2003.0,0.0,149.3...|-0.26550931174716114|\n|  11.7|(8,[0,4,6,7],[200...|  -0.528585070027704|\n|7119.6|[2004.0,0.0,3501....|  1.9409291696477728|\n| 377.3|[2004.0,0.0,287.2...| -0.4015638111431736|\n|1574.0|[2004.0,0.0,835.9...|0.014208455014150438|\n| 405.5|[2004.0,0.0,132.3...| -0.3917662195114893|\n|1068.0|[2004.0,0.0,687.9...|-0.16159230263238247|\n| 434.8|(8,[0,3],[2004.0,...|-0.38158645231970395|\n+------+--------------------+--------------------+\nonly showing top 20 rows\n\n"
    }
   ],
   "source": [
    "final_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now scaling our features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+------+--------------------+--------------------+--------------------+\n|Total_|            features|        scaled_total|     scaled_features|\n+------+--------------------+--------------------+--------------------+\n|5990.4|[2003.0,0.0,2899....|  1.5486085855875653|[533.851570764226...|\n| 432.6|[2003.0,0.0,223.1...| -0.3823508034399062|[533.851570764226...|\n|1525.0|[2003.0,0.0,756.4...|-0.00281572902671...|[533.851570764226...|\n| 257.5|[2003.0,0.0,89.9,...|-0.44318620396146335|[533.851570764226...|\n| 858.5|[2003.0,0.0,519.4...|-0.23437937521528493|[533.851570764226...|\n| 438.8|(8,[0,3],[2003.0,...|-0.38019672301024515|(8,[0,3],[533.851...|\n| 792.8|[2003.0,0.0,602.3...|-0.25720567912314507|[533.851570764226...|\n| 452.6|[2003.0,0.0,249.7...|-0.37540215689261236|[533.851570764226...|\n| 581.3|[2003.0,0.0,224.6...| -0.3306876163607769|[533.851570764226...|\n| 651.3|[2003.0,0.0,233.6...| -0.3063673534452486|[533.851570764226...|\n| 104.3|[2003.0,0.0,0.0,1...| -0.4964128365137338|[533.851570764226...|\n|3724.4|[2003.0,0.0,228.0...|   0.761326931779179|[533.851570764226...|\n| 768.9|[2003.0,0.0,149.3...|-0.26550931174716114|[533.851570764226...|\n|  11.7|(8,[0,4,6,7],[200...|  -0.528585070027704|(8,[0,4,6,7],[533...|\n|7119.6|[2004.0,0.0,3501....|  1.9409291696477728|[534.118096760613...|\n| 377.3|[2004.0,0.0,287.2...| -0.4015638111431736|[534.118096760613...|\n|1574.0|[2004.0,0.0,835.9...|0.014208455014150438|[534.118096760613...|\n| 405.5|[2004.0,0.0,132.3...| -0.3917662195114893|[534.118096760613...|\n|1068.0|[2004.0,0.0,687.9...|-0.16159230263238247|[534.118096760613...|\n| 434.8|(8,[0,3],[2004.0,...|-0.38158645231970395|(8,[0,3],[534.118...|\n+------+--------------------+--------------------+--------------------+\nonly showing top 20 rows\n\n"
    }
   ],
   "source": [
    "final_data = scaler.fit(final_data).transform(final_data)\n",
    "final_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keep only two decimals after point in scaled_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+------------+--------------------+\n|scaled_total|     scaled_features|\n+------------+--------------------+\n|        1.55|[533.851570764226...|\n|       -0.38|[533.851570764226...|\n|         0.0|[533.851570764226...|\n|       -0.44|[533.851570764226...|\n|       -0.23|[533.851570764226...|\n|       -0.38|(8,[0,3],[533.851...|\n|       -0.26|[533.851570764226...|\n|       -0.38|[533.851570764226...|\n|       -0.33|[533.851570764226...|\n|       -0.31|[533.851570764226...|\n|        -0.5|[533.851570764226...|\n|        0.76|[533.851570764226...|\n|       -0.27|[533.851570764226...|\n|       -0.53|(8,[0,4,6,7],[533...|\n|        1.94|[534.118096760613...|\n|        -0.4|[534.118096760613...|\n|        0.01|[534.118096760613...|\n|       -0.39|[534.118096760613...|\n|       -0.16|[534.118096760613...|\n|       -0.38|(8,[0,3],[534.118...|\n+------------+--------------------+\nonly showing top 20 rows\n\n"
    }
   ],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "final_data = final_data.withColumn('scaled_total', round(lr_pred['scaled_total'],2))\n",
    "final_data = final_data.drop('Total_','features')\n",
    "final_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+-------+-------------------+\n|summary|       scaled_total|\n+-------+-------------------+\n|  count|                125|\n|   mean|0.00799999999999995|\n| stddev| 1.0582418468628108|\n|    min|              -0.53|\n|    max|               6.93|\n+-------+-------------------+\n\n"
    }
   ],
   "source": [
    "train_data, test_data = final_data.randomSplit([0.7,0.3])\n",
    "train_data.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression,RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol='scaled_features',labelCol='scaled_total')\n",
    "rf = RandomForestRegressor(featuresCol='scaled_features',labelCol='scaled_total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = lr.fit(train_data)\n",
    "rf_model = rf.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pred = lr_model.transform(test_data)\n",
    "rf_pred = rf_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+------------+--------------------+----------+\n|scaled_total|     scaled_features|prediction|\n+------------+--------------------+----------+\n|       -0.53|[536.250304731714...|      -0.5|\n|       -0.52|[534.651148753388...|      -0.5|\n|       -0.52|[535.717252738939...|     -0.37|\n|       -0.51|[536.516830728101...|     -0.48|\n|       -0.48|[534.118096760613...|     -0.49|\n|       -0.48|[536.250304731714...|      -0.3|\n|       -0.45|[535.184200746163...|     -0.18|\n|       -0.44|[533.851570764226...|     -0.27|\n|       -0.44|[536.516830728101...|     -0.31|\n|       -0.43|[536.516830728101...|     -0.24|\n|       -0.42|[535.184200746163...|      -0.2|\n|       -0.42|[535.717252738939...|     -0.36|\n|       -0.42|[535.983778735326...|     -0.37|\n|       -0.41|[534.651148753388...|     -0.38|\n|       -0.41|[535.450726742551...|     -0.32|\n|       -0.41|[535.450726742551...|      -0.2|\n|        -0.4|[534.118096760613...|     -0.16|\n|       -0.39|[534.118096760613...|     -0.11|\n|       -0.38|[533.851570764226...|       0.0|\n|       -0.38|[533.851570764226...|     -0.36|\n+------------+--------------------+----------+\nonly showing top 20 rows\n\n"
    }
   ],
   "source": [
    "rf_pred = rf_pred.withColumn('prediction',round(rf_pred['prediction'],2))\n",
    "lr_pred = lr_pred.withColumn('prediction',round(lr_pred['prediction'],2))\n",
    "rf_pred.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator,BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = BinaryClassificationEvaluator(labelCol='scaled_total',rawPredictionCol='prediction')\n",
    "acc_eval = MulticlassClassificationEvaluator(labelCol='scaled_total',metricName='accuracy',predictionCol='prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.8856209150326798"
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval.evaluate(lr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.9738562091503268"
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval.evaluate(rf_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.017543859649122806"
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_eval.evaluate(rf_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}