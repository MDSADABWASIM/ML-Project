{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitbaseconda0633cb87348e4e1f83d9c32f4c55bf11",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark streaming basics project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "### Note on  Streaming\n",
    "Streaming is something that is rapidly advancing and changing fast, there are multiple new libraries every year, new and different services always popping up, and what is in this notebook may or may not apply to you. Maybe your looking for something specific on Kafka, or maybe you are looking for streaming about twitter, in which case Spark might be overkill for what you really want. Realistically speaking each situation is going to require a customized solution and this course is never going to be able to supply a one size fits all solution. Because of this, I wanted to point out some great resources for Python and Spark StreamingL\n",
    "\n",
    "* [The Official Documentation is great. This should be your first go to.](http://spark.apache.org/docs/latest/streaming-programming-guide.html#spark-streaming-programming-guide)\n",
    "\n",
    "* [Fantastic Guide to Spark Streaming with Kafka](https://www.rittmanmead.com/blog/2017/01/getting-started-with-spark-streaming-with-python-and-kafka/)\n",
    "\n",
    "* [Another Spark Streaming Example with Geo Plotting](http://nbviewer.jupyter.org/github/ibm-cds-labs/spark.samples/blob/master/notebook/DashDB%20Twitter%20Car%202015%20Python%20Notebook.ipynb)\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark has pretty well known Streaming Capabilities, if streaming is something you've found yourself needing at work then you are probably familiar with some of these concepts already, in which case you may find it more useful to jump straight to the official documentation here:\n",
    "\n",
    "http://spark.apache.org/docs/latest/streaming-programming-guide.html#spark-streaming-programming-guide\n",
    "\n",
    "It is really a great guide, but keep in mind some of the features are restricted to Scala at this time (Spark 2.1), hopefully they will be expanded to the Python API in the future!\n",
    "\n",
    "For those of you new to Spark Streaming, let's get started with a classic example, streaming Twitter! Twitter is a great source for streaming because its something most people already have an intuitive understanding of, you can visit the site yourself, and a lot of streaming technology has come out of Twitter as a company. You don't access to the entire \"firehose\" of twitter without paying for it, but that would be too much for us to handle anyway, so we'll be more than fine with the freely available API access.\n",
    "\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's discuss SparkStreaming!\n",
    "\n",
    "Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map, reduce, join and window. Finally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Sparkâ€™s machine learning and graph processing algorithms on data streams.\n",
    "\n",
    "<img src='http://spark.apache.org/docs/latest/img/streaming-arch.png'/>\n",
    "\n",
    "Keep in mind that a few of these Streamiing Capabilities are limited when it comes to Python, you'll need to reference the documentation for the most up to date information. Also the streaming contexts tend to follow more along with the older RDD syntax, so a few things might seem different than what we are used to seeing, keep that in mind, you'll definitely want to have a good understanding of lambda expressions before continuing with this!\n",
    "\n",
    "There are SparkSQL modules for streaming: \n",
    "\n",
    "http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=streaming#module-pyspark.sql.streaming\n",
    "\n",
    "But they are all still listed as experimental, so instead of showing you somethign that might break in the future, we'll stick to the RDD methods (which is what the documentation also currently shows for streaming).\n",
    "\n",
    "Internally, it works as follows. Spark Streaming receives live input data streams and divides the data into batches, which are then processed by the Spark engine to generate the final stream of results in batches.\n",
    "\n",
    "<img src='http://spark.apache.org/docs/latest/img/streaming-flow.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple local example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode,split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('spark_stream').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame representing the stream of input lines from connection to localhost:9999\n",
    "lines = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "# Split the lines into words\n",
    "words = lines.select(\n",
    "    explode(\n",
    "        split(lines.value, \" \")\n",
    "    ).alias('word')\n",
    ")\n",
    "\n",
    "# Generate running word count\n",
    "wordCounts = words.groupBy('word').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we open up a Unix terminal and type:\n",
    "\n",
    "         $ nc -lk 9999\n",
    "     $ hello world any text you want\n",
    "     \n",
    "With this running run the line below, then type Ctrl+C to terminate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start running the query that prints the running counts to the console\n",
    "query = wordCounts.writeStream.outputMode('complete').format('console').start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "' will print like this based on words we enter\\n\\n-------------------------------------------                                     \\nBatch: 0\\n-------------------------------------------\\n+----+-----+\\n|word|count|\\n+----+-----+\\n+----+-----+\\n\\n-------------------------------------------                                     \\nBatch: 1\\n-------------------------------------------\\n+------+-----+\\n|  word|count|\\n+------+-----+\\n|hadoop|    1|\\n+------+-----+\\n\\n-------------------------------------------                                     \\nBatch: 2\\n-------------------------------------------\\n+------+-----+\\n|  word|count|\\n+------+-----+\\n|apache|    1|\\n|hadoop|    1|\\n+------+-----+\\n\\n-------------------------------------------                                     \\nBatch: 3\\n-------------------------------------------\\n+------+-----+\\n|  word|count|\\n+------+-----+\\n|apache|    1|\\n| spark|    1|\\n|hadoop|    2|\\n+------+-----+\\n\\n-------------------------------------------                                     \\nBatch: 4\\n-------------------------------------------\\n+------+-----+\\n|  word|count|\\n+------+-----+\\n|apache|    2|\\n| spark|    1|\\n|hadoop|    2|\\n+------+-----+\\n'"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' will print like this based on words we enter\n",
    "\n",
    "-------------------------------------------                                     \n",
    "Batch: 0\n",
    "-------------------------------------------\n",
    "+----+-----+\n",
    "|word|count|\n",
    "+----+-----+\n",
    "+----+-----+\n",
    "\n",
    "-------------------------------------------                                     \n",
    "Batch: 1\n",
    "-------------------------------------------\n",
    "+------+-----+\n",
    "|  word|count|\n",
    "+------+-----+\n",
    "|hadoop|    1|\n",
    "+------+-----+\n",
    "\n",
    "-------------------------------------------                                     \n",
    "Batch: 2\n",
    "-------------------------------------------\n",
    "+------+-----+\n",
    "|  word|count|\n",
    "+------+-----+\n",
    "|apache|    1|\n",
    "|hadoop|    1|\n",
    "+------+-----+\n",
    "\n",
    "-------------------------------------------                                     \n",
    "Batch: 3\n",
    "-------------------------------------------\n",
    "+------+-----+\n",
    "|  word|count|\n",
    "+------+-----+\n",
    "|apache|    1|\n",
    "| spark|    1|\n",
    "|hadoop|    2|\n",
    "+------+-----+\n",
    "\n",
    "-------------------------------------------                                     \n",
    "Batch: 4\n",
    "-------------------------------------------\n",
    "+------+-----+\n",
    "|  word|count|\n",
    "+------+-----+\n",
    "|apache|    2|\n",
    "| spark|    1|\n",
    "|hadoop|    2|\n",
    "+------+-----+\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}