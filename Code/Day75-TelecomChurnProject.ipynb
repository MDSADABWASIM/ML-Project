{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitbaseconda0633cb87348e4e1f83d9c32f4c55bf11",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Telecom customer churn project\n",
    "predictive analytics use churn prediction models that predict customer churn by assessing their propensity of risk to      churn. Since these models generate a small prioritized list of potential defectors, they are effective at focusing         customer retention marketing programs on the subset of the customer base who are most vulnerable to churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('telecom_churn').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('../datasets/telecom_data.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "root\n |-- state: string (nullable = true)\n |-- account length: integer (nullable = true)\n |-- area code: integer (nullable = true)\n |-- phone number: string (nullable = true)\n |-- international plan: string (nullable = true)\n |-- voice mail plan: string (nullable = true)\n |-- number vmail messages: integer (nullable = true)\n |-- total day minutes: double (nullable = true)\n |-- total day calls: integer (nullable = true)\n |-- total day charge: double (nullable = true)\n |-- total eve minutes: double (nullable = true)\n |-- total eve calls: integer (nullable = true)\n |-- total eve charge: double (nullable = true)\n |-- total night minutes: double (nullable = true)\n |-- total night calls: integer (nullable = true)\n |-- total night charge: double (nullable = true)\n |-- total intl minutes: double (nullable = true)\n |-- total intl calls: integer (nullable = true)\n |-- total intl charge: double (nullable = true)\n |-- customer service calls: integer (nullable = true)\n |-- churn: boolean (nullable = true)\n\n"
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+-----+------------------+-----+\n|state|international plan|churn|\n+-----+------------------+-----+\n|   KS|                no|false|\n|   OH|                no|false|\n|   NJ|                no|false|\n|   OH|               yes|false|\n|   OK|               yes|false|\n|   AL|               yes|false|\n|   MA|                no|false|\n|   MO|               yes|false|\n|   LA|                no|false|\n|   WV|               yes|false|\n|   IN|                no| true|\n|   RI|                no|false|\n|   IA|                no|false|\n|   MT|                no|false|\n|   IA|                no|false|\n|   NY|                no| true|\n|   ID|                no|false|\n|   VT|                no|false|\n|   VA|                no|false|\n|   TX|                no|false|\n+-----+------------------+-----+\nonly showing top 20 rows\n\n"
    }
   ],
   "source": [
    "df.select('state','international plan','churn').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+-----+-----+\n|churn|count|\n+-----+-----+\n| true|  483|\n|false| 2850|\n+-----+-----+\n\n"
    }
   ],
   "source": [
    "df.groupBy('churn').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for null data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function use to print feature with null values and null count \n",
    "def null_value_count(df):\n",
    "  null_columns_counts = []\n",
    "  numRows = df.count()\n",
    "  for k in df.columns:\n",
    "    nullRows = df.where(df[k].isNull()).count()\n",
    "    if(nullRows > 0):\n",
    "      temp = k,nullRows\n",
    "      null_columns_counts.append(temp)\n",
    "  return(null_columns_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "There are 0 null data points in dataset\n"
    }
   ],
   "source": [
    "null_values = null_value_count(df)\n",
    "print(\"There are %s null data points in dataset\"%len(null_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Next: Converting the Boolean 'churn' column to String, so that we can convert String to Numerical column using String indexer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "root\n |-- state: string (nullable = true)\n |-- account length: integer (nullable = true)\n |-- area code: integer (nullable = true)\n |-- phone number: string (nullable = true)\n |-- international plan: string (nullable = true)\n |-- voice mail plan: string (nullable = true)\n |-- number vmail messages: integer (nullable = true)\n |-- total day minutes: double (nullable = true)\n |-- total day calls: integer (nullable = true)\n |-- total day charge: double (nullable = true)\n |-- total eve minutes: double (nullable = true)\n |-- total eve calls: integer (nullable = true)\n |-- total eve charge: double (nullable = true)\n |-- total night minutes: double (nullable = true)\n |-- total night calls: integer (nullable = true)\n |-- total night charge: double (nullable = true)\n |-- total intl minutes: double (nullable = true)\n |-- total intl calls: integer (nullable = true)\n |-- total intl charge: double (nullable = true)\n |-- customer service calls: integer (nullable = true)\n |-- churn: string (nullable = true)\n\n"
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "df = df.withColumn(\"churn\", df[\"churn\"].cast(StringType()))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now churn is a String column*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering:\n",
    "    converting categorical columns to numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(df) for column in [\"state\",\"voice mail plan\",\"churn\",\"international plan\"]]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "df = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+-----------+---------------------+-----------+------------------------+\n|state_index|voice mail plan_index|churn_index|international plan_index|\n+-----------+---------------------+-----------+------------------------+\n|       17.0|                  1.0|        0.0|                     0.0|\n|        5.0|                  1.0|        0.0|                     0.0|\n|       18.0|                  0.0|        0.0|                     0.0|\n|        5.0|                  0.0|        0.0|                     1.0|\n|       34.0|                  0.0|        0.0|                     1.0|\n|        3.0|                  0.0|        0.0|                     1.0|\n|       24.0|                  1.0|        0.0|                     0.0|\n|       28.0|                  0.0|        0.0|                     1.0|\n|       47.0|                  0.0|        0.0|                     0.0|\n|        0.0|                  1.0|        0.0|                     1.0|\n|       15.0|                  0.0|        1.0|                     0.0|\n|       25.0|                  0.0|        0.0|                     0.0|\n|       49.0|                  0.0|        0.0|                     0.0|\n|       19.0|                  0.0|        0.0|                     0.0|\n|       49.0|                  0.0|        0.0|                     0.0|\n|        2.0|                  0.0|        1.0|                     0.0|\n|       10.0|                  1.0|        0.0|                     0.0|\n|       11.0|                  0.0|        0.0|                     0.0|\n|        8.0|                  1.0|        0.0|                     0.0|\n|       13.0|                  0.0|        0.0|                     0.0|\n+-----------+---------------------+-----------+------------------------+\nonly showing top 20 rows\n\n"
    }
   ],
   "source": [
    "# Let's see our new columns\n",
    "df.select('state_index','voice mail plan_index','churn_index','international plan_index').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection and Formating Feature:\n",
    "    Formating feature to provide it to spark's MLlib library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\n",
    " 'account length',\n",
    " 'number vmail messages',\n",
    " 'total day minutes',\n",
    " 'total day calls',\n",
    " 'total day charge',\n",
    " 'total eve minutes',\n",
    " 'total eve calls',\n",
    " 'total eve charge',\n",
    " 'total night minutes',\n",
    " 'total night calls',\n",
    " 'total night charge',\n",
    " 'total intl minutes',\n",
    " 'total intl calls',\n",
    " 'total intl charge',\n",
    " 'customer service calls',\n",
    " 'state_index',\n",
    " 'voice mail plan_index',\n",
    " 'international plan_index'],outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "root\n |-- state: string (nullable = true)\n |-- account length: integer (nullable = true)\n |-- area code: integer (nullable = true)\n |-- phone number: string (nullable = true)\n |-- international plan: string (nullable = true)\n |-- voice mail plan: string (nullable = true)\n |-- number vmail messages: integer (nullable = true)\n |-- total day minutes: double (nullable = true)\n |-- total day calls: integer (nullable = true)\n |-- total day charge: double (nullable = true)\n |-- total eve minutes: double (nullable = true)\n |-- total eve calls: integer (nullable = true)\n |-- total eve charge: double (nullable = true)\n |-- total night minutes: double (nullable = true)\n |-- total night calls: integer (nullable = true)\n |-- total night charge: double (nullable = true)\n |-- total intl minutes: double (nullable = true)\n |-- total intl calls: integer (nullable = true)\n |-- total intl charge: double (nullable = true)\n |-- customer service calls: integer (nullable = true)\n |-- churn: string (nullable = true)\n |-- state_index: double (nullable = false)\n |-- voice mail plan_index: double (nullable = false)\n |-- churn_index: double (nullable = false)\n |-- international plan_index: double (nullable = false)\n |-- features: vector (nullable = true)\n\n"
    }
   ],
   "source": [
    "df = assembler.transform(df)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now we have a new \"Features\" column with all features assembled as a vector*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selecting our label and features column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = df.select('churn_index','features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+-----------+--------------------+\n|churn_index|            features|\n+-----------+--------------------+\n|        0.0|[128.0,25.0,265.1...|\n|        0.0|[107.0,26.0,161.6...|\n|        0.0|[137.0,0.0,243.4,...|\n|        0.0|[84.0,0.0,299.4,7...|\n|        0.0|[75.0,0.0,166.7,1...|\n|        0.0|[118.0,0.0,223.4,...|\n|        0.0|[121.0,24.0,218.2...|\n|        0.0|[147.0,0.0,157.0,...|\n|        0.0|[117.0,0.0,184.5,...|\n|        0.0|[141.0,37.0,258.6...|\n|        1.0|[65.0,0.0,129.1,1...|\n|        0.0|[74.0,0.0,187.7,1...|\n|        0.0|[168.0,0.0,128.8,...|\n|        0.0|[95.0,0.0,156.6,8...|\n|        0.0|[62.0,0.0,120.7,7...|\n|        1.0|[161.0,0.0,332.9,...|\n|        0.0|[85.0,27.0,196.4,...|\n|        0.0|[93.0,0.0,190.7,1...|\n|        0.0|[76.0,33.0,189.7,...|\n|        0.0|[73.0,0.0,224.4,9...|\n+-----------+--------------------+\nonly showing top 20 rows\n\n"
    }
   ],
   "source": [
    "final_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the data:\n",
    "    to feed it to ML algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol='features',outputCol='scaled_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = scaler.fit(final_data).transform(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+-----------+--------------------+--------------------+\n|churn_index|            features|     scaled_features|\n+-----------+--------------------+--------------------+\n|        0.0|[128.0,25.0,265.1...|[3.21429510105554...|\n|        0.0|[107.0,26.0,161.6...|[2.68694981103861...|\n|        0.0|[137.0,0.0,243.4,...|[3.44030022534851...|\n|        0.0|[84.0,0.0,299.4,7...|[2.10938116006770...|\n|        0.0|[75.0,0.0,166.7,1...|[1.88337603577473...|\n|        0.0|[118.0,0.0,223.4,...|[2.96317829628557...|\n|        0.0|[121.0,24.0,218.2...|[3.03851333771656...|\n|        0.0|[147.0,0.0,157.0,...|[3.69141703011847...|\n|        0.0|[117.0,0.0,184.5,...|[2.93806661580858...|\n|        0.0|[141.0,37.0,258.6...|[3.54074694725649...|\n|        1.0|[65.0,0.0,129.1,1...|[1.63225923100476...|\n|        0.0|[74.0,0.0,187.7,1...|[1.85826435529773...|\n|        0.0|[168.0,0.0,128.8,...|[4.21876232013540...|\n|        0.0|[95.0,0.0,156.6,8...|[2.38560964531466...|\n|        0.0|[62.0,0.0,120.7,7...|[1.55692418957377...|\n|        1.0|[161.0,0.0,332.9,...|[4.04298055679642...|\n|        0.0|[85.0,27.0,196.4,...|[2.13449284054469...|\n|        0.0|[93.0,0.0,190.7,1...|[2.33538628436066...|\n|        0.0|[76.0,33.0,189.7,...|[1.90848771625172...|\n|        0.0|[73.0,0.0,224.4,9...|[1.83315267482074...|\n+-----------+--------------------+--------------------+\nonly showing top 20 rows\n\n"
    }
   ],
   "source": [
    "final_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "root\n |-- churn_index: double (nullable = false)\n |-- scaled_features: vector (nullable = true)\n\n"
    }
   ],
   "source": [
    "final_data = final_data.select('churn_index','scaled_features')\n",
    "final_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split of dataset:\n",
    "    To Train and Test our ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting to 70 % and 30 % as train and test data respectively\n",
    "\n",
    "train_data, test_data = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+-------+-------------------+\n|summary|        churn_index|\n+-------+-------------------+\n|  count|                993|\n|   mean|0.14400805639476336|\n| stddev|0.35127482109457975|\n|    min|                0.0|\n|    max|                1.0|\n+-------+-------------------+\n\n"
    }
   ],
   "source": [
    "test_data.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model building:\n",
    "    Trying two algorithms Gradient Boosting Tree and Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier,RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = GBTClassifier(featuresCol='scaled_features',labelCol='churn_index',seed=101) \n",
    "rfc = RandomForestClassifier(featuresCol='scaled_features',labelCol='churn_index',numTrees=50,seed=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gbc = gbc.fit(train_data)\n",
    "model_rfc = rfc.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions using testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_gbc = model_gbc.transform(test_data)\n",
    "result_rfc = model_rfc.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+-----------+--------------------+--------------------+--------------------+----------+\n|churn_index|     scaled_features|       rawPrediction|         probability|prediction|\n+-----------+--------------------+--------------------+--------------------+----------+\n|        0.0|[0.02511168047699...|[47.2313086366029...|[0.94462617273205...|       0.0|\n|        0.0|[0.02511168047699...|[46.198533156258,...|[0.92397066312516...|       0.0|\n|        0.0|[0.07533504143098...|[46.5567606335217...|[0.93113521267043...|       0.0|\n|        0.0|[0.07533504143098...|[47.3363718376515...|[0.94672743675303...|       0.0|\n|        0.0|[0.10044672190798...|[41.6463342815863...|[0.83292668563172...|       0.0|\n|        0.0|[0.15067008286197...|[46.6382207657665...|[0.93276441531533...|       0.0|\n|        0.0|[0.15067008286197...|[46.6560355550944...|[0.93312071110188...|       0.0|\n|        0.0|[0.17578176333897...|[45.6082638566766...|[0.91216527713353...|       0.0|\n|        0.0|[0.17578176333897...|[46.8517343782814...|[0.93703468756562...|       0.0|\n|        0.0|[0.20089344381597...|[45.8260779588945...|[0.91652155917789...|       0.0|\n|        0.0|[0.22600512429296...|[47.4206835145808...|[0.94841367029161...|       0.0|\n|        0.0|[0.22600512429296...|[47.3394763099776...|[0.94678952619955...|       0.0|\n|        0.0|[0.27622848524696...|[47.1955894226803...|[0.94391178845360...|       0.0|\n|        0.0|[0.27622848524696...|[47.2366529016171...|[0.94473305803234...|       0.0|\n|        0.0|[0.30134016572395...|[43.4928193637509...|[0.86985638727501...|       0.0|\n|        0.0|[0.32645184620095...|[46.3845490298797...|[0.92769098059759...|       0.0|\n|        0.0|[0.32645184620095...|[47.2864420271781...|[0.94572884054356...|       0.0|\n|        0.0|[0.37667520715494...|[43.3217772717543...|[0.86643554543508...|       0.0|\n|        0.0|[0.40178688763194...|[47.2065216258608...|[0.94413043251721...|       0.0|\n|        0.0|[0.40178688763194...|[47.1437659717874...|[0.94287531943574...|       0.0|\n+-----------+--------------------+--------------------+--------------------+----------+\nonly showing top 20 rows\n\n"
    }
   ],
   "source": [
    "result_rfc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_eval = MulticlassClassificationEvaluator(labelCol='churn_index',metricName='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Accuracy of Gradient Boosting Tree: 0.9556898288016112\nAccuracy of Random Forest: 0.9154078549848943\n"
    }
   ],
   "source": [
    "acc_gbc = acc_eval.evaluate(result_gbc)\n",
    "acc_rfc = acc_eval.evaluate(result_rfc)\n",
    "\n",
    "print(\"Accuracy of Gradient Boosting Tree: \"+str(acc_gbc))\n",
    "print(\"Accuracy of Random Forest: \"+str(acc_rfc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting gives us 95 % accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}